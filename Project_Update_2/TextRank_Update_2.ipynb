{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8C6Ylg-a8lV",
        "outputId": "b3173f08-a311-43ea-e995-5184f10cb05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip -qq install spacy pytextrank contractions\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import spacy\n",
        "import pytextrank\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "import nltk\n",
        "\n",
        "# Ensuring necessary NLTK data is downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Loading spaCy and pyTextRank model\n",
        "print(\"Loading spaCy and pyTextRank model...\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"textrank\")\n",
        "nlp.max_length = 1500000  # Set maximum length to handle larger texts\n",
        "\n",
        "# Initializing lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocessing function with text cleaning and normalization\n",
        "def preprocess_text(text, max_length=1000000):\n",
        "    \"\"\"Cleans and preprocesses text, with an optional length limit.\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"  # Handling missing text by returning empty string\n",
        "    text = contractions.fix(text.lower())  # Normalizing contractions and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Removing special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Removing extra whitespace\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text[:max_length]\n",
        "\n",
        "# Function to remove rows with missing data in specified columns\n",
        "def remove_missing_data(dataset, columns):\n",
        "    \"\"\"Removes rows with missing data from specified columns in a dataset.\"\"\"\n",
        "    return dataset.filter(lambda x: all(x[col] is not None for col in columns))\n",
        "\n",
        "# Loading and cleaning dataset\n",
        "def load_and_clean_data():\n",
        "    \"\"\"Loads and cleans the dataset, returning validation and test splits.\"\"\"\n",
        "    validation_data = load_dataset(\"ragha92/FNS_Summarization\", split=\"validation\")\n",
        "    test_data = load_dataset(\"ragha92/FNS_Summarization\", split=\"test\")\n",
        "\n",
        "    columns = ['Annual Reports', 'Gold Summaries']\n",
        "    validation_data = remove_missing_data(validation_data, columns)\n",
        "    test_data = remove_missing_data(test_data, columns)\n",
        "\n",
        "    return validation_data, test_data\n",
        "\n",
        "# Summarizing text using TextRank\n",
        "def summarize_text_textrank(article, top_n=5):\n",
        "    \"\"\"Generates a summary using TextRank on a given article.\"\"\"\n",
        "    doc = nlp(article)\n",
        "    summary = [str(sent) for sent in doc._.textrank.summary(limit_sentences=top_n)]\n",
        "    return ' '.join(summary)\n",
        "\n",
        "# Evaluation function using ROUGE and BLEU\n",
        "def evaluate_model_textrank(dataset, set_name):\n",
        "    \"\"\"Evaluates the model on a dataset using ROUGE and BLEU metrics.\"\"\"\n",
        "    print(f\"Evaluating {set_name} dataset using TextRank...\")\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bleu': []}\n",
        "    smooth = SmoothingFunction().method4\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        article = preprocess_text(item['Annual Reports'])\n",
        "        gold_summary = preprocess_text(item['Gold Summaries'])\n",
        "\n",
        "        if article:\n",
        "            generated_summary = summarize_text_textrank(article)\n",
        "            score = scorer.score(gold_summary, generated_summary)\n",
        "            scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "            scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "            scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "            # BLEU score with smoothing\n",
        "            reference = gold_summary.split()\n",
        "            candidate = generated_summary.split()\n",
        "            bleu_score = sentence_bleu([reference], candidate, smoothing_function=smooth)\n",
        "            scores['bleu'].append(bleu_score)\n",
        "\n",
        "    # Calculating average scores\n",
        "    avg_rouge1 = sum(scores['rouge1']) / len(scores['rouge1'])\n",
        "    avg_rouge2 = sum(scores['rouge2']) / len(scores['rouge2'])\n",
        "    avg_rougeL = sum(scores['rougeL']) / len(scores['rougeL'])\n",
        "    avg_bleu = sum(scores['bleu']) / len(scores['bleu'])\n",
        "\n",
        "    return avg_rouge1, avg_rouge2, avg_rougeL, avg_bleu\n",
        "\n",
        "# Main function to run the evaluation\n",
        "def main():\n",
        "    print(\"Loading and cleaning datasets...\")\n",
        "    validation_data, test_data = load_and_clean_data()\n",
        "\n",
        "    print(\"Evaluating on the validation set...\")\n",
        "    avg_rouge1, avg_rouge2, avg_rougeL, avg_bleu = evaluate_model_textrank(validation_data, \"validation\")\n",
        "    print(f\"Validation ROUGE-1: {avg_rouge1}, ROUGE-2: {avg_rouge2}, ROUGE-L: {avg_rougeL}, BLEU: {avg_bleu}\")\n",
        "\n",
        "    print(\"Evaluating on the test dataset...\")\n",
        "    avg_rouge1, avg_rouge2, avg_rougeL, avg_bleu = evaluate_model_textrank(test_data, \"test\")\n",
        "    print(f\"Test Set ROUGE-1: {avg_rouge1}\")\n",
        "    print(f\"Test Set ROUGE-2: {avg_rouge2}\")\n",
        "    print(f\"Test Set ROUGE-L: {avg_rougeL}\")\n",
        "    print(f\"Test Set BLEU: {avg_bleu}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH8KnmGGbBzH",
        "outputId": "8e8cbc13-e591-494e-8896-984219892045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages\n",
            "Loading spaCy and pyTextRank model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and cleaning datasets...\n",
            "Evaluating on the validation set...\n",
            "Evaluating validation dataset using TextRank...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 257/257 [43:26<00:00, 10.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ROUGE-1: 0.09427128906111303, ROUGE-2: 0.06314497963095687, ROUGE-L: 0.0646837550850029, BLEU: 0.034209738463307784\n",
            "Evaluating on the test dataset...\n",
            "Evaluating test dataset using TextRank...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 259/259 [41:04<00:00,  9.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set ROUGE-1: 0.09632594059871431\n",
            "Test Set ROUGE-2: 0.06739978070004861\n",
            "Test Set ROUGE-L: 0.0701921449531556\n",
            "Test Set BLEU: 0.03755184845554105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}